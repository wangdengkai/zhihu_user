# 					目标:爬取知乎网站用户信息

## 环境:python3,scrapy,mongodb

实施过程:

​	1 	分析网站上某个用户的个人页面,从个人页面中获取关注他的人,和他关注的人的请求地址

​	2	请求他关注的人和他关注的人的每个人个个人页面,获取每个用户的基本信息,不包含发表的言论数据等,因为那样内容太多了.原理相同,如果需要的话,可以在这里随时稍微增加一些功能就好.

​	3	解析响应后,运用mongodb进行存储.

关键难点:	

​	1 分析出用户信息,关注者,被关注者相关的连接.参考zhihuanaly

​	2  知乎有反扒措施,要降低爬取速率,要添加请求头,要用代理.



本案列参考:https://www.cnblogs.com/zhaof/p/7215617.html

特此声明:这个案列仅供学习使用,请勿做其他非法行为.



​	